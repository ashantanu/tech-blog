{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to LangChain Output Parsers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have fiddled around with ChatGPT or [LangChain](https://python.langchain.com/) yet, you would have realised how difficult it is to get the outcome we expect everytime. For whatever reason, we might expect a list, or a table and it would output a lot of text that we never asked for.\n",
    "\n",
    "Let's follow an example to see what is the issue we usually face with LLMs and how we can use Output Parsers to resolve it. But first, some setup!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# run this as %%bash cell or in terminal\n",
    "conda create --name llm\n",
    "conda activate llm\n",
    "pip install langchain\n",
    "pip install openai\n",
    "pip install duckduckgo-search\n",
    "\n",
    "# for notebook kernel (I always forget this)\n",
    "conda install ipykernel\n",
    "ipython kernel install --user --name=llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# yes you need to get the openAI key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "\n",
    "# need to make a free account at https://serper.dev/ for this key\n",
    "os.environ[\"SERPER_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to Our Example\n",
    "Suppose you want to create a LLM chain to suggest a dinner restaurant along with details like it's address, a short description and the most popular dish. Let's see what happens when we try to do that.\n",
    "\n",
    "*Ideally I'd like to have langchain search it online and then hook up with the right chain, but let's try to keep things simple for now.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"restaurant_name\": \"Joe's Shanghai\",\n",
      "  \"address\": \"9 Pell St, New York, NY 10013\",\n",
      "  \"description\": \"Joe's Shanghai is a bustling restaurant known for its authentic Chinese cuisine and lively atmosphere.\",\n",
      "  \"popular_dish\": \"Soup Dumplings\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "template_string = \"\"\"You are a travel agent who helps guests with popular\n",
    "food places for their favourite cuisines. You are helping a guest who \n",
    "wants to eat {cuisine} food in {city}. Recommend a restaurant for them.\n",
    "Output the name of the restuarant, address, a single short sentence \n",
    "description, and the most popular dish. Return the output in a JSON format.\"\"\"\n",
    "prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        HumanMessagePromptTemplate.from_template(template_string)  \n",
    "    ],\n",
    "    input_variables=[\"cuisine\", \"city\"],\n",
    ")\n",
    "food_llm = ChatOpenAI()\n",
    "food_chain = LLMChain(prompt=prompt, llm=food_llm)\n",
    "response = food_chain.run(cuisine=\"Chinese\", city=\"New York\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this response seems like a very nice JSON, but it is actually a string with formatting like below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"restaurant_name\": \"Joe\\'s Shanghai\",\\n  \"address\": \"9 Pell St, New York, NY 10013\",\\n  \"description\": \"Joe\\'s Shanghai is a bustling restaurant known for its authentic Chinese cuisine and lively atmosphere.\",\\n  \"popular_dish\": \"Soup Dumplings\"\\n}'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which makes it a nightmare to parse. And God know what different formats LLM would output, each requiring it's own parser. LOL no way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello Structured Output Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will cover one of the simpler output parsers called [`Structure Output Parser`](https://python.langchain.com/docs/modules/model_io/output_parsers/structured).It is used to make sure that the output of the model is a data structure with text fields. Let's see it in action!\n",
    "\n",
    "We need the below ingredients\n",
    "- a `ResponseSchema` for each of the text fields we want in our output\n",
    "- put all response schemas together into an array\n",
    "- create a `StructureOutputParser` object with the above array\n",
    "- this will create format instructions to guide the LLM towards the expected output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"restaurant_name\": string  // Name of the restaurant suggested to the user\n",
      "\t\"address\": string  // address of the suggested restaurant\n",
      "\t\"description\": string  // a short description of the restaurant\n",
      "\t\"popular_dish\": string  // a popular dish at the restaurant\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain.prompts import (PromptTemplate,\n",
    "                               ChatPromptTemplate,\n",
    "                               HumanMessagePromptTemplate)\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# create a response schema\n",
    "\n",
    "response_schemas = [\n",
    "    ResponseSchema(name=\"restaurant_name\",\n",
    "                   description=\"Name of the restaurant suggested to the user\"),\n",
    "    ResponseSchema(name=\"address\",\n",
    "                   description=\"address of the suggested restaurant\"),\n",
    "    ResponseSchema(name=\"description\",\n",
    "                   description=\"a short description of the restaurant\"),\n",
    "    ResponseSchema(name=\"popular_dish\",\n",
    "                   description=\"a popular dish at the restaurant\"),\n",
    "]\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now let's plug this into our chain above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "\t\"restaurant_name\": \"Nom Wah Tea Parlor\",\n",
      "\t\"address\": \"13 Doyers St, New York, NY 10013\",\n",
      "\t\"description\": \"A historic dim sum parlor serving traditional Chinese dishes in Chinatown.\",\n",
      "\t\"popular_dish\": \"Shrimp and snow pea leaf dumplings\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "template_string = \"\"\"You are a travel agent who helps guests with popular\n",
    "food places for their favourite cuisines. You are helping a guest who \n",
    "wants to eat {cuisine} food in {city}. Recommend a restaurant for them.\n",
    "Output the name of the restuarant, address, a single short sentence \n",
    "description, and the most popular dish.\n",
    "{format_instructions}\"\"\"\n",
    "prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        HumanMessagePromptTemplate.from_template(template_string)  \n",
    "    ],\n",
    "    input_variables=[\"cuisine\", \"city\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions}\n",
    ")\n",
    "food_llm = ChatOpenAI()\n",
    "food_chain = LLMChain(prompt=prompt, llm=food_llm)\n",
    "response = food_chain.run(cuisine=\"Chinese\", city=\"New York\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'restaurant_name': 'Nom Wah Tea Parlor',\n",
       " 'address': '13 Doyers St, New York, NY 10013',\n",
       " 'description': 'A historic dim sum parlor serving traditional Chinese dishes in Chinatown.',\n",
       " 'popular_dish': 'Shrimp and snow pea leaf dumplings'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and now we parse the output\n",
    "output_parser.parse(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay! We have successfully parsed the output of the LLM model and avoided writings them ourselves. So much for being lazy! (but this helps with a lot of handcrafting and rule writing, phew)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CHAIN IT UP!\n",
    "I mean, what's the point of writing chains if I can't put the output parser in chain. Let's go!\n",
    "\n",
    "To achieve this, we simply add `output_parser=output_parser` to our chain and voila"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'restaurant_name': \"Joe's Shanghai\",\n",
       " 'address': '9 Pell St, New York, NY 10013',\n",
       " 'description': 'A popular Chinese restaurant known for its soup dumplings.',\n",
       " 'popular_dish': 'Soup Dumplings'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        HumanMessagePromptTemplate.from_template(template_string)  \n",
    "    ],\n",
    "    input_variables=[\"cuisine\", \"city\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    ")\n",
    "food_llm = ChatOpenAI()\n",
    "food_chain = LLMChain(prompt=prompt, llm=food_llm, output_parser=output_parser)\n",
    "response = food_chain.run(cuisine=\"Chinese\", city=\"New York\")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "We covered the simplest OutputParser in this blog and show how to use it for nuanced control over output. It helps keep templates clean, and avoid writing unnecessary parsers. I expect these to still fail sometimes (still LLMs after all) but they are much more robust than the previous approach.\n",
    "\n",
    "# What Next?\n",
    "But LangChain is SO MUCH more powerful! The way more cooler parsers is the Pydantic parser. It is a bit more involved, but it is worth it. We will cover it in the next blog. Till then, **keep chaining! ⛓️ Cheers! 🫖**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "- Sam Witteveen's [video](https://www.youtube.com/watch?v=UVn2NroKQCw&ab_channel=SamWitteveen) and [notebook](https://colab.research.google.com/drive/1D3i-4yiPvRmUX7PWWiat7iNX-HoggKS9?usp=sharing)\n",
    "- [LangChain](https://python.langchain.com/) official documentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
