{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Tweet Graveyard\n",
    "*a new place to bury my tweet bookmarks, but think ML*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Andrej Karpathy beatifully covers why llama.cpp works.\n",
    "\n",
    "<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">&quot;How is LLaMa.cpp possible?&quot; <br>great post by <a href=\"https://twitter.com/finbarrtimbers?ref_src=twsrc%5Etfw\">@finbarrtimbers</a> <a href=\"https://t.co/yF43inlY87\">https://t.co/yF43inlY87</a><br><br>llama.cpp surprised many people (myself included) with how quickly you can run large LLMs on small computers, e.g. 7B runs @ ~16 tok/s on a MacBook. Wait don&#39;t you need supercomputers to workâ€¦ <a href=\"https://t.co/EIp9iPkZ6x\">pic.twitter.com/EIp9iPkZ6x</a></p>&mdash; Andrej Karpathy (@karpathy) <a href=\"https://twitter.com/karpathy/status/1691571869051445433?ref_src=twsrc%5Etfw\">August 15, 2023</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
